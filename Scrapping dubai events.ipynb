{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3788ad55-bda2-426c-a2b2-c67c646ff5f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting urllib3<3.0,>=2.5.0 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio~=0.30.0 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.6.15 (from selenium)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting typing_extensions~=4.14.0 (from selenium)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Collecting sortedcontainers (from trio~=0.30.0->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from trio~=0.30.0->selenium) (2.0.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: pycparser in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\preet\\envs\\sentiment_env\\lib\\site-packages (from requests->webdriver-manager) (3.4.2)\n",
      "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 7.3/9.6 MB 41.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 39.9 MB/s eta 0:00:00\n",
      "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, urllib3, typing_extensions, python-dotenv, pysocks, outcome, certifi, webdriver-manager, trio, trio-websocket, selenium\n",
      "\n",
      "   --- ------------------------------------  1/12 [wsproto]\n",
      "  Attempting uninstall: urllib3\n",
      "   --- ------------------------------------  1/12 [wsproto]\n",
      "    Found existing installation: urllib3 2.4.0\n",
      "   --- ------------------------------------  1/12 [wsproto]\n",
      "   ------ ---------------------------------  2/12 [urllib3]\n",
      "    Uninstalling urllib3-2.4.0:\n",
      "   ------ ---------------------------------  2/12 [urllib3]\n",
      "      Successfully uninstalled urllib3-2.4.0\n",
      "   ------ ---------------------------------  2/12 [urllib3]\n",
      "   ------ ---------------------------------  2/12 [urllib3]\n",
      "   ------ ---------------------------------  2/12 [urllib3]\n",
      "   ------ ---------------------------------  2/12 [urllib3]\n",
      "  Attempting uninstall: typing_extensions\n",
      "   ------ ---------------------------------  2/12 [urllib3]\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "   ------ ---------------------------------  2/12 [urllib3]\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "   ------ ---------------------------------  2/12 [urllib3]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ---------- -----------------------------  3/12 [typing_extensions]\n",
      "   ------------- --------------------------  4/12 [python-dotenv]\n",
      "   -------------------- -------------------  6/12 [outcome]\n",
      "  Attempting uninstall: certifi\n",
      "   -------------------- -------------------  6/12 [outcome]\n",
      "    Found existing installation: certifi 2025.4.26\n",
      "   -------------------- -------------------  6/12 [outcome]\n",
      "    Uninstalling certifi-2025.4.26:\n",
      "   -------------------- -------------------  6/12 [outcome]\n",
      "      Successfully uninstalled certifi-2025.4.26\n",
      "   -------------------- -------------------  6/12 [outcome]\n",
      "   -------------------------- -------------  8/12 [webdriver-manager]\n",
      "   -------------------------- -------------  8/12 [webdriver-manager]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------ ---------  9/12 [trio]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ------------------------------------ --- 11/12 [selenium]\n",
      "   ---------------------------------------- 12/12 [selenium]\n",
      "\n",
      "Successfully installed certifi-2025.8.3 outcome-1.3.0.post0 pysocks-1.7.1 python-dotenv-1.1.1 selenium-4.35.0 sortedcontainers-2.4.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 urllib3-2.5.0 webdriver-manager-4.0.2 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41edaa13-a699-458b-ad91-ba7480b4a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, json, hashlib\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dtparse\n",
    "import pytz\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import gzip, io\n",
    "\n",
    "\n",
    "# Config\n",
    "DUBAI_TZ = pytz.timezone(\"Asia/Dubai\")\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "REQ_TIMEOUT = 20\n",
    "PAUSE = 0.4  \n",
    "\n",
    "# Eventbrite \n",
    "EVENTBRITE_TOKEN = os.getenv(\"EVENTBRITE_TOKEN\", \"WJS7O2YA33T2S3UC5U4I\").strip()\n",
    "USE_EVENTBRITE = bool(EVENTBRITE_TOKEN) and EVENTBRITE_TOKEN.upper() != \"WJS7O2YA33T2S3UC5U4I\"\n",
    "\n",
    "\n",
    "# Helpers\n",
    "def now_dubai() -> datetime:\n",
    "    return datetime.now(DUBAI_TZ)\n",
    "\n",
    "def aware_dt(s: Optional[str]) -> Optional[datetime]:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        d = dtparse.parse(s)\n",
    "        if d.tzinfo is None:\n",
    "            d = DUBAI_TZ.localize(d)\n",
    "        else:\n",
    "            d = d.astimezone(DUBAI_TZ)\n",
    "        return d\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def to_iso(d: Optional[datetime]) -> Optional[str]:\n",
    "    return d.isoformat() if d else None\n",
    "\n",
    "def make_uid(parts) -> str:\n",
    "    base = \"|\".join([p or \"\" for p in parts])\n",
    "    return hashlib.sha1(base.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def norm_row(source, source_id, title, start_dt, end_dt,\n",
    "             url=None, venue=None, address=None, city=\"Dubai\", country=\"UAE\",\n",
    "             status=\"scheduled\", category=None, organizer=None, price=None, image=None) -> Dict:\n",
    "    uid = make_uid([source, str(source_id or \"\"), title or \"\", to_iso(start_dt) or \"\"])\n",
    "    return {\n",
    "        \"uid\": uid,\n",
    "        \"source\": source,\n",
    "        \"source_id\": str(source_id or \"\"),\n",
    "        \"title\": (title or \"\").strip(),\n",
    "        \"start\": to_iso(start_dt),\n",
    "        \"end\": to_iso(end_dt),\n",
    "        \"status\": status,\n",
    "        \"url\": url,\n",
    "        \"venue\": venue,\n",
    "        \"address\": address,\n",
    "        \"city\": city,\n",
    "        \"country\": country,\n",
    "        \"category\": category,\n",
    "        \"organizer\": organizer,\n",
    "        \"price\": price,\n",
    "        \"image\": image,\n",
    "        \"ingested_at\": to_iso(now_dubai()),\n",
    "    }\n",
    "\n",
    "def keep_future_or_ongoing(row: Dict, assume_future_if_missing: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Relaxed by default so you see rows even if dates don't parse.\n",
    "    Set assume_future_if_missing=False to strictly keep only future/ongoing rows.\n",
    "    \"\"\"\n",
    "    now = now_dubai()\n",
    "    s = aware_dt(row.get(\"start\")) if row.get(\"start\") else None\n",
    "    e = aware_dt(row.get(\"end\")) if row.get(\"end\") else None\n",
    "    if s and s >= now: return True\n",
    "    if e and e >= now: return True\n",
    "    if s and s.date() == now.date(): return True\n",
    "    return assume_future_if_missing and (s is None and e is None)\n",
    "\n",
    "def dedupe(rows: List[Dict]) -> List[Dict]:\n",
    "    seen, out = set(), []\n",
    "    for r in rows:\n",
    "        if r[\"uid\"] in seen:\n",
    "            continue\n",
    "        seen.add(r[\"uid\"]); out.append(r)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Eventbrite \n",
    "\n",
    "def fetch_eventbrite_dubai() -> List[Dict]:\n",
    "    rows = []\n",
    "    if not USE_EVENTBRITE:\n",
    "        return rows\n",
    "    url = \"https://www.eventbriteapi.com/v3/events/search/\"\n",
    "    params = {\n",
    "        \"q\": \"Dubai\",\n",
    "        \"location.address\": \"Dubai\",\n",
    "        \"expand\": \"venue,category,organizer,logo\",\n",
    "        \"sort_by\": \"date\",\n",
    "        \"page\": 1,\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"Bearer {EVENTBRITE_TOKEN}\"}\n",
    "    while True:\n",
    "        r = requests.get(url, params=params, headers=headers, timeout=REQ_TIMEOUT)\n",
    "        if r.status_code != 200:\n",
    "            print(\"Eventbrite HTTP\", r.status_code, \"- stopping.\")\n",
    "            break\n",
    "        data = r.json()\n",
    "        evs = data.get(\"events\", []) or []\n",
    "        if not evs:\n",
    "            break\n",
    "        for ev in evs:\n",
    "            title = (ev.get(\"name\") or {}).get(\"text\")\n",
    "            start = aware_dt((ev.get(\"start\") or {}).get(\"utc\"))\n",
    "            end   = aware_dt((ev.get(\"end\") or {}).get(\"utc\"))\n",
    "            venue = (ev.get(\"venue\") or {}).get(\"name\")\n",
    "            address = None\n",
    "            if ev.get(\"venue\") and (ev[\"venue\"].get(\"address\")):\n",
    "                a = ev[\"venue\"][\"address\"]\n",
    "                address = \", \".join([a.get(k) for k in [\"address_1\",\"address_2\",\"city\",\"region\",\"postal_code\"] if a.get(k)])\n",
    "            url_e = ev.get(\"url\")\n",
    "            category = (ev.get(\"category\") or {}).get(\"name\")\n",
    "            organizer = (ev.get(\"organizer\") or {}).get(\"name\")\n",
    "            image = (ev.get(\"logo\") or {}).get(\"url\")\n",
    "            rows.append(norm_row(\"eventbrite\", ev.get(\"id\"), title, start, end, url_e, venue, address,\n",
    "                                 category=category, organizer=organizer, image=image))\n",
    "        params[\"page\"] += 1\n",
    "        time.sleep(PAUSE)\n",
    "    print(\"Eventbrite rows:\", len(rows))\n",
    "    return rows\n",
    "\n",
    "\n",
    "# VisitDubai (skip gracefully if non-200)\n",
    "\n",
    "def parse_visitdubai_card(card) -> Optional[Dict]:\n",
    "    a = card.select_one(\"a[href]\")\n",
    "    if not a: return None\n",
    "    href = a[\"href\"]\n",
    "    url = \"https://www.visitdubai.com\" + href if href.startswith(\"/\") else href\n",
    "    title_el = card.select_one(\"h3, h2, .card__title, .c-card__title, [data-test='title']\")\n",
    "    title = title_el.get_text(\" \", strip=True) if title_el else \"Event\"\n",
    "\n",
    "    date_el = card.select_one(\".card__date, .event-card__date, .c-card__date, .date, [data-test='date']\")\n",
    "    date_text = date_el.get_text(\" \", strip=True) if date_el else None\n",
    "\n",
    "    start_dt = end_dt = None\n",
    "    if date_text:\n",
    "        parts = re.split(r\"\\s*[-–—]\\s*\", date_text)\n",
    "        try:\n",
    "            if len(parts) == 2:\n",
    "                start_dt = aware_dt(parts[0] + \" \" + str(datetime.now().year))\n",
    "                end_dt = aware_dt(parts[1])\n",
    "                if end_dt and start_dt and end_dt < start_dt:\n",
    "                    start_dt = aware_dt(parts[0] + \" \" + str(end_dt.year))\n",
    "            else:\n",
    "                start_dt = aware_dt(date_text)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    venue_el = card.select_one(\".event-card__venue, .venue, .c-card__subtitle, [data-test='venue']\")\n",
    "    venue = venue_el.get_text(\" \", strip=True) if venue_el else None\n",
    "\n",
    "    img_el = card.select_one(\"img[src], img[data-src]\")\n",
    "    image = (img_el.get(\"src\") or img_el.get(\"data-src\")) if img_el else None\n",
    "\n",
    "    return norm_row(\"visitdubai\", url, title, start_dt, end_dt, url, venue, None, image=image)\n",
    "\n",
    "def fetch_visitdubai(max_pages: int = 3) -> List[Dict]:\n",
    "    rows = []\n",
    "    base = \"https://www.visitdubai.com/en/whats-on/dubai-events\"\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = base if page == 1 else f\"{base}?page={page}\"\n",
    "        r = requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT)\n",
    "        if r.status_code != 200:\n",
    "            print(\"VisitDubai HTTP\", r.status_code, \"- skipping.\")\n",
    "            break\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        cards = soup.select(\"article, .event-card, .c-card, li a[href*='/en/whats-on/'], a[href*='/en/whats-on/']\")\n",
    "        got = 0\n",
    "        for c in cards:\n",
    "            row = parse_visitdubai_card(c)\n",
    "            if row:\n",
    "                rows.append(row); got += 1\n",
    "        print(f\"VisitDubai page {page}: parsed {got} rows\")\n",
    "        time.sleep(PAUSE)\n",
    "    print(\"VisitDubai rows:\", len(rows))\n",
    "    return rows\n",
    "\n",
    "\n",
    "# Platinumlist \n",
    "ROBOTS = [\n",
    "    \"https://platinumlist.net/robots.txt\",\n",
    "    \"https://dubai.platinumlist.net/robots.txt\",\n",
    "]\n",
    "SITEMAP_SEEDS = [\n",
    "    \"https://platinumlist.net/sitemap.xml\",\n",
    "    \"https://platinumlist.net/sitemap_index.xml\",\n",
    "    \"https://dubai.platinumlist.net/sitemap.xml\",\n",
    "    \"https://dubai.platinumlist.net/sitemap_index.xml\",\n",
    "]\n",
    "\n",
    "def _fetch_bytes(url: str) -> Optional[bytes]:\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT, allow_redirects=True)\n",
    "        if r.status_code == 200 and r.content:\n",
    "            return r.content\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _fetch_text_or_gzip(url: str) -> Optional[str]:\n",
    "    \"\"\"Fetch XML or XML.GZ and return text.\"\"\"\n",
    "    b = _fetch_bytes(url)\n",
    "    if not b:\n",
    "        return None\n",
    "    # try decompress if .gz, otherwise decode\n",
    "    if url.lower().split(\"?\")[0].endswith(\".gz\"):\n",
    "        try:\n",
    "            with gzip.GzipFile(fileobj=io.BytesIO(b)) as gz:\n",
    "                return gz.read().decode(\"utf-8\", errors=\"replace\")\n",
    "        except Exception:\n",
    "            try:\n",
    "                return b.decode(\"utf-8\", errors=\"replace\")\n",
    "            except Exception:\n",
    "                return None\n",
    "    else:\n",
    "        try:\n",
    "            return b.decode(\"utf-8\", errors=\"replace\")\n",
    "        except Exception:\n",
    "            # sometimes servers send gzipped content with wrong extension\n",
    "            try:\n",
    "                with gzip.GzipFile(fileobj=io.BytesIO(b)) as gz:\n",
    "                    return gz.read().decode(\"utf-8\", errors=\"replace\")\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "def _parse_sitemap_locs(xml_text: str) -> List[str]:\n",
    "    # Extract <loc>...</loc>\n",
    "    locs = re.findall(r\"<loc>\\s*([^<]+)\\s*</loc>\", xml_text, flags=re.I)\n",
    "    return [u.strip() for u in locs if u.strip()]\n",
    "\n",
    "def _harvest_from_robots() -> List[str]:\n",
    "    \"\"\"Read robots.txt to discover all listed sitemaps.\"\"\"\n",
    "    out = []\n",
    "    for rob in ROBOTS:\n",
    "        txt = _fetch_text_or_gzip(rob)\n",
    "        if not txt:\n",
    "            continue\n",
    "        for line in txt.splitlines():\n",
    "            line = line.strip()\n",
    "            if line.lower().startswith(\"sitemap:\"):\n",
    "                url = line.split(\":\", 1)[1].strip()\n",
    "                if url:\n",
    "                    out.append(url)\n",
    "    # add fallbacks\n",
    "    out.extend(SITEMAP_SEEDS)\n",
    "    # dedupe\n",
    "    seen, uniq = set(), []\n",
    "    for u in out:\n",
    "        if u not in seen:\n",
    "            seen.add(u); uniq.append(u)\n",
    "    return uniq\n",
    "\n",
    "def _expand_sitemaps(seed_urls: List[str], max_depth: int = 2) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively expand sitemap indexes into concrete sitemap files and page URLs.\n",
    "    Returns a flat list of all <loc> values found.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    visited = set()\n",
    "    queue = [(u, 0) for u in seed_urls]\n",
    "\n",
    "    while queue:\n",
    "        url, depth = queue.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        xml = _fetch_text_or_gzip(url)\n",
    "        if not xml:\n",
    "            continue\n",
    "\n",
    "        locs = _parse_sitemap_locs(xml)\n",
    "        if not locs:\n",
    "            continue\n",
    "\n",
    "        results.extend(locs)\n",
    "\n",
    "        \n",
    "        if depth < max_depth:\n",
    "            nested = [u for u in locs if (\"sitemap\" in u.lower()) or u.lower().endswith((\".xml\", \".xml.gz\"))]\n",
    "            for n in nested:\n",
    "                queue.append((n, depth + 1))\n",
    "\n",
    "        time.sleep(PAUSE)\n",
    "\n",
    "    # dedupe\n",
    "    seen, uniq = set(), []\n",
    "    for u in results:\n",
    "        if u not in seen:\n",
    "            seen.add(u); uniq.append(u)\n",
    "    return uniq\n",
    "\n",
    "def harvest_platinum_sitemaps(max_links: int = 300) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover all sitemaps via robots.txt (+ seeds), expand them, and return\n",
    "    concrete event detail URLs (/event/).\n",
    "    \"\"\"\n",
    "    seed = _harvest_from_robots()\n",
    "    expanded = _expand_sitemaps(seed, max_depth=2)\n",
    "\n",
    "    \n",
    "    page_like = [u for u in expanded if \"/event/\" in u.lower()]\n",
    "    # Normalize \n",
    "    clean = []\n",
    "    for u in page_like:\n",
    "        if u.startswith(\"//\"): u = \"https:\" + u\n",
    "        if u.startswith(\"/\"):  u = \"https://platinumlist.net\" + u\n",
    "        clean.append(u)\n",
    "\n",
    "   \n",
    "    prefer = [u for u in clean if \"dubai\" in u.lower()]\n",
    "    rest   = [u for u in clean if \"dubai\" not in u.lower()]\n",
    "    ordered = prefer + rest\n",
    "\n",
    "    # Dedupe & cap\n",
    "    seen, uniq = set(), []\n",
    "    for u in ordered:\n",
    "        if u not in seen:\n",
    "            seen.add(u); uniq.append(u)\n",
    "    return uniq[:max_links]\n",
    "\n",
    "def _first_event_jsonld_from_html(html: str) -> Optional[dict]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup.select('script[type=\"application/ld+json\"]'):\n",
    "        raw = (tag.string or tag.text or \"\").strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "        try:\n",
    "            blob = json.loads(raw)\n",
    "            items = blob if isinstance(blob, list) else [blob]\n",
    "            for d in items:\n",
    "                if isinstance(d, dict) and str(d.get(\"@type\",\"\")).lower() == \"event\":\n",
    "                    return d\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def parse_platinum_detail_fast(url: str) -> Optional[Dict]:\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        data = _first_event_jsonld_from_html(r.text)\n",
    "        if not data:\n",
    "            return None\n",
    "        title = data.get(\"name\") or \"Event\"\n",
    "        start_dt = aware_dt(data.get(\"startDate\")) if isinstance(data.get(\"startDate\"), str) else None\n",
    "        end_dt   = aware_dt(data.get(\"endDate\"))   if isinstance(data.get(\"endDate\"), str)   else None\n",
    "\n",
    "        venue = None\n",
    "        loc = data.get(\"location\")\n",
    "        if isinstance(loc, dict):\n",
    "            venue = loc.get(\"name\") or (loc.get(\"address\") if isinstance(loc.get(\"address\"), str) else None)\n",
    "\n",
    "        img = data.get(\"image\")\n",
    "        image = img[0] if isinstance(img, list) and img else (img if isinstance(img, str) else None)\n",
    "\n",
    "        return norm_row(\"platinumlist\", url, title, start_dt, end_dt, url, venue, None, image=image)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_platinumlist_via_sitemaps(max_pages: int = 2, per_page_limit: int = 120) -> List[Dict]:\n",
    "\n",
    "    max_links = per_page_limit * max_pages\n",
    "    links = harvest_platinum_sitemaps(max_links=max_links)\n",
    "    print(\"Sitemap event links:\", len(links))\n",
    "\n",
    "    out = []\n",
    "    if not links:\n",
    "        return out\n",
    "\n",
    "    \n",
    "    workers = min(12, max(2, len(links)))\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        futs = {ex.submit(parse_platinum_detail_fast, url): url for url in links}\n",
    "        for fut in as_completed(futs):\n",
    "            row = fut.result()\n",
    "            if row:\n",
    "                out.append(row)\n",
    "\n",
    "    print(\"Parsed event rows:\", len(out))\n",
    "    return dedupe(out)\n",
    "\n",
    "# Runner\n",
    "\n",
    "def run(csv_path: str = \"dubai_events.csv\",\n",
    "        include_eventbrite: bool = USE_EVENTBRITE,\n",
    "        include_visitdubai: bool = False,      # default False because you saw 404s earlier\n",
    "        include_platinumlist: bool = True,\n",
    "        pages: int = 2,\n",
    "        relaxed_filter: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape events and write to CSV. Returns the DataFrame.\n",
    "    - relaxed_filter=True keeps rows even if dates are missing/unparsable (so you see output).\n",
    "    \"\"\"\n",
    "    all_rows: List[Dict] = []\n",
    "\n",
    "    if include_eventbrite:\n",
    "        try:\n",
    "            all_rows.extend(fetch_eventbrite_dubai())\n",
    "        except Exception as e:\n",
    "            print(\"Eventbrite fetch failed:\", repr(e))\n",
    "\n",
    "    if include_visitdubai:\n",
    "        try:\n",
    "            all_rows.extend(fetch_visitdubai(max_pages=pages))\n",
    "        except Exception as e:\n",
    "            print(\"VisitDubai fetch failed:\", repr(e))\n",
    "\n",
    "    if include_platinumlist:\n",
    "        try:\n",
    "            all_rows.extend(fetch_platinumlist_via_sitemaps(max_pages=pages))\n",
    "        except Exception as e:\n",
    "            print(\"Platinumlist fetch failed:\", repr(e))\n",
    "\n",
    "    all_rows = dedupe(all_rows)\n",
    "    filtered = [r for r in all_rows if keep_future_or_ongoing(r, assume_future_if_missing=relaxed_filter)]\n",
    "\n",
    "    df = pd.DataFrame(filtered, columns=[\n",
    "        \"uid\",\"source\",\"source_id\",\"title\",\"start\",\"end\",\"status\",\"url\",\"venue\",\n",
    "        \"address\",\"city\",\"country\",\"category\",\"organizer\",\"price\",\"image\",\"ingested_at\"\n",
    "    ])\n",
    "\n",
    "    def _safe_dt(x):\n",
    "        try: return dtparse.parse(x) if pd.notna(x) else None\n",
    "        except Exception: return None\n",
    "\n",
    "    if not df.empty:\n",
    "        df[\"start_dt_sort\"] = df[\"start\"].apply(_safe_dt)\n",
    "        df = df.sort_values(by=[\"start_dt_sort\",\"title\"], ascending=[True, True]).drop(columns=[\"start_dt_sort\"], errors=\"ignore\")\n",
    "\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved {len(df)} events to {csv_path}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e386bc-8166-4b0c-a62d-0a979698218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sitemap event links: 2\n",
      "Parsed event rows: 0\n",
      "Saved 0 events to dubai_events.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>source</th>\n",
       "      <th>source_id</th>\n",
       "      <th>title</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>status</th>\n",
       "      <th>url</th>\n",
       "      <th>venue</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>category</th>\n",
       "      <th>organizer</th>\n",
       "      <th>price</th>\n",
       "      <th>image</th>\n",
       "      <th>ingested_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [uid, source, source_id, title, start, end, status, url, venue, address, city, country, category, organizer, price, image, ingested_at]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 0\n",
      "By source:\n",
      " Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "df = run(\n",
    "    csv_path=\"dubai_events.csv\",\n",
    "    include_eventbrite=False,\n",
    "    include_visitdubai=False,   \n",
    "    include_platinumlist=True,\n",
    "    pages=2,                    \n",
    "    relaxed_filter=True\n",
    ")\n",
    "\n",
    "from IPython.display import display\n",
    "display(df.head(20))\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"By source:\\n\", df[\"source\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64eb0d-156b-4e06-907c-9ea8289819a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentiment_env]",
   "language": "python",
   "name": "conda-env-sentiment_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
