{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6b2bb9-74d8-4e0f-a553-a0fcf9144a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded single file: C:\\Users\\Admin\\Downloads\\events merged.csv\n",
      "Rows before merge: 5034\n",
      "Rows after dedup:  3559\n",
      "Saved: C:\\Users\\Admin\\Downloads\\events_clean_dedup.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "INPUT_PATH = Path(r\"C:\\Users\\Admin\\Downloads\\events merged.csv\")                 \n",
    "\n",
    "def read_any_csv(path: Path) -> pd.DataFrame:\n",
    "    for enc in [\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_inputs(input_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"If input_path is a dir: load & concat all CSVs. If file: load that one.\"\"\"\n",
    "    if input_path.is_file():\n",
    "        df = read_any_csv(input_path)\n",
    "        df[\"_source_file\"] = input_path.name\n",
    "        print(f\"Loaded single file: {input_path}\")\n",
    "        return df\n",
    "    elif input_path.is_dir():\n",
    "        csvs = sorted(input_path.glob(\"*.csv\"))\n",
    "        if not csvs:\n",
    "            raise FileNotFoundError(f\"No CSV files found in folder: {input_path}\")\n",
    "        frames: List[pd.DataFrame] = []\n",
    "        for p in csvs:\n",
    "            try:\n",
    "                df = read_any_csv(p)\n",
    "                df[\"_source_file\"] = p.name\n",
    "                frames.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipped unreadable file: {p.name} ({e})\")\n",
    "        if not frames:\n",
    "            raise RuntimeError(\"No readable CSVs found.\")\n",
    "        print(f\"Merging {len(frames)} CSV files from: {input_path}\")\n",
    "        return pd.concat(frames, ignore_index=True, sort=True)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Path does not exist: {input_path}\")\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(r\"[\\s\\-]+\", \"_\", regex=True)\n",
    "        .str.replace(r\"[^\\w_]\", \"\", regex=True)\n",
    "        .str.lower()\n",
    "    )\n",
    "\n",
    "    colmap = {\n",
    "        \"title\": [\"title\",\"event_title\",\"name\",\"eventname\",\"event_name\"],\n",
    "        \"description\": [\"description\",\"details\",\"summary\"],\n",
    "        \"start\": [\"start\",\"start_time\",\"start_datetime\",\"startdate\",\"start_date\",\"event_start\",\"begins\"],\n",
    "        \"end\": [\"end\",\"end_time\",\"end_datetime\",\"enddate\",\"end_date\",\"event_end\",\"ends\"],\n",
    "        \"venue\": [\"venue\",\"venue_name\",\"place\"],\n",
    "        \"city\": [\"city\",\"town\"],\n",
    "        \"country\": [\"country\"],\n",
    "        \"address\": [\"address\",\"location\",\"address_line\"],\n",
    "        \"url\": [\"url\",\"link\",\"event_url\",\"permalink\"],\n",
    "        \"source\": [\"source\",\"source_site\",\"origin\"],\n",
    "        \"price\": [\"price\",\"cost\",\"amount\"],\n",
    "        \"currency\": [\"currency\",\"price_currency\"],\n",
    "        \"category\": [\"category\",\"type\",\"event_type\"],\n",
    "        \"organizer\": [\"organizer\",\"host\"],\n",
    "        \"latitude\": [\"latitude\",\"lat\"],\n",
    "        \"longitude\": [\"longitude\",\"lon\",\"lng\"],\n",
    "    }\n",
    "    for target, cands in colmap.items():\n",
    "        if target not in df.columns:\n",
    "            for c in cands:\n",
    "                if c in df.columns:\n",
    "                    df[target] = df[c]; break\n",
    "        if target not in df.columns:\n",
    "            df[target] = np.nan\n",
    "\n",
    "    # Clean text & nulls\n",
    "    for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        s = df[c].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "        s = s.mask(s.str.lower().isin([\"nan\",\"none\",\"null\",\"na\",\"\"]))\n",
    "        df[c] = s\n",
    "\n",
    "    # Datetimes to UTC\n",
    "    for t in [\"start\",\"end\"]:\n",
    "        if t in df.columns:\n",
    "            df[t] = pd.to_datetime(df[t], errors=\"coerce\", utc=True)\n",
    "\n",
    "    if \"url\" in df.columns:\n",
    "        s = df[\"url\"].astype(str).str.strip()\n",
    "        s = s.str.replace(r\"/+$\", \"\", regex=True).str.lower()\n",
    "        df[\"url\"] = s.mask(s.isin([\"\",\"nan\"]))\n",
    "\n",
    "   \n",
    "    for col in [\"venue\",\"city\",\"country\",\"address\",\"category\",\"organizer\"]:\n",
    "        if col in df.columns:\n",
    "            s = df[col].astype(\"string\")\n",
    "            df[col] = s.where(s.isna(), s.str.title())\n",
    "\n",
    "    if \"currency\" in df.columns:\n",
    "        s = df[\"currency\"].astype(\"string\")\n",
    "        df[\"currency\"] = s.where(s.isna(), s.str.upper())\n",
    "\n",
    "    # Numeric price extraction\n",
    "    if \"price\" in df.columns:\n",
    "        df[\"price_raw\"] = df[\"price\"]\n",
    "        cleaned = (\n",
    "            df[\"price\"].astype(str)\n",
    "            .str.replace(r\"[^\\d\\.,\\-]\", \"\", regex=True)\n",
    "            .str.replace(\",\", \".\", regex=False)\n",
    "        )\n",
    "        def to_float(s):\n",
    "            if pd.isna(s): return np.nan\n",
    "            s = str(s)\n",
    "            parts = s.split(\".\")\n",
    "            if len(parts) > 2:\n",
    "                s = \"\".join(parts[:-1]) + \".\" + parts[-1]\n",
    "            try: return float(s)\n",
    "            except: return np.nan\n",
    "        df[\"price_value\"] = cleaned.apply(to_float)\n",
    "\n",
    "    #  dedup\n",
    "    df[\"std_title\"] = df[\"title\"].astype(str).str.lower().str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    df[\"std_city\"]  = (df[\"city\"].astype(str).str.lower().str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "                       if \"city\" in df.columns else np.nan)\n",
    "    df[\"start_date\"] = df[\"start\"].dt.date if \"start\" in df.columns else pd.NaT\n",
    "    return df\n",
    "\n",
    "def build_dedup_key(df: pd.DataFrame) -> pd.Series:\n",
    "    url_key = df[\"url\"]\n",
    "    fallback = (\n",
    "        df[\"std_title\"].fillna(\"\")\n",
    "        + \" | \" + df[\"start_date\"].astype(str).fillna(\"\")\n",
    "        + \" | \" + df[\"std_city\"].fillna(\"\")\n",
    "    )\n",
    "    return pd.Series(\n",
    "        np.where(url_key.notna(), \"url::\" + url_key.astype(str), \"tsc::\" + fallback.astype(str)),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "def pick_best_record(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    score_cols = [\"description\",\"venue\",\"address\",\"city\",\"country\",\"price_value\",\"category\",\"organizer\"]\n",
    "    for c in score_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    score = df[score_cols].notna().sum(axis=1) + df[\"start\"].notna().astype(int)\n",
    "    df = df.assign(_score=score)\n",
    "    keep_idx = df.groupby(\"dedup_key\")[\"_score\"].idxmax()\n",
    "    return df.loc[keep_idx].drop(columns=[\"_score\"]).sort_index()\n",
    "\n",
    "#  Run \n",
    "raw = load_inputs(INPUT_PATH)\n",
    "print(\"Rows before merge:\", len(raw))\n",
    "\n",
    "norm = normalize_columns(raw)\n",
    "norm[\"dedup_key\"] = build_dedup_key(norm)\n",
    "\n",
    "dedup = pick_best_record(norm)\n",
    "print(\"Rows after dedup: \", len(dedup))\n",
    "\n",
    "preferred = [\n",
    "    \"title\",\"description\",\"start\",\"end\",\"venue\",\"address\",\"city\",\"country\",\n",
    "    \"category\",\"organizer\",\"price_raw\",\"price_value\",\"currency\",\"url\",\"source\",\n",
    "    \"latitude\",\"longitude\",\"start_date\",\"dedup_key\",\"_source_file\"\n",
    "]\n",
    "cols = [c for c in preferred if c in dedup.columns] + [c for c in dedup.columns if c not in preferred]\n",
    "\n",
    "# Output \n",
    "out_dir = INPUT_PATH.parent if INPUT_PATH.is_file() else INPUT_PATH\n",
    "out_path = out_dir / \"events_clean_dedup.csv\"\n",
    "dedup[cols].to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a36df-a57a-4c72-a64e-5e1b25cecf75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentiment_env]",
   "language": "python",
   "name": "conda-env-sentiment_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
